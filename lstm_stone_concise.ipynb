{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56958fcf-f363-421c-9200-8f73383f17f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T07:52:31.972294Z",
     "start_time": "2022-04-02T07:52:27.694843Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import jieba\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8176086-d128-4353-8c05-20dac9359ea7",
   "metadata": {},
   "source": [
    "# 通用module， timer， animator， accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2da9810d-7c1c-4909-a07f-0ddec5a0c6e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T07:52:37.741980Z",
     "start_time": "2022-04-02T07:52:37.713881Z"
    }
   },
   "outputs": [],
   "source": [
    "class Timer: #@save\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Defined in :numref:`subsec_linear_model`\"\"\"\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "    \n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"Set the axes for matplotlib.\n",
    "\n",
    "    Defined in :numref:`sec_calculus`\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()\n",
    "    \n",
    "    \n",
    "class Animator:\n",
    "    \"\"\"For plotting data in animation.\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        \"\"\"Defined in :numref:`sec_softmax_scratch`\"\"\"\n",
    "        # Incrementally plot multiple lines\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # Use a lambda function to capture arguments\n",
    "        self.config_axes = lambda: set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # Add multiple data points into the figure\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "class Accumulator:\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        \"\"\"Defined in :numref:`sec_softmax_scratch`\"\"\"\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2ef0e8-547c-4744-8697-9031f483979a",
   "metadata": {},
   "source": [
    "# nlp vocab与读文件的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552603de-8d9c-41b0-9f27-6e97eddf1fe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T07:52:41.504208Z",
     "start_time": "2022-04-02T07:52:41.470306Z"
    },
    "code_folding": [
     19
    ]
   },
   "outputs": [],
   "source": [
    "def read_chinese_file(path):\n",
    "    \"\"\"Load the chinese file into a list of text lines.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('\\s', ' ', line).strip() for line in lines]\n",
    "\n",
    "\n",
    "def tokenize(lines):\n",
    "    \"\"\"根据jieba分词将lines分成tokens\"\"\"\n",
    "    return [jieba.lcut(line) for line in lines]\n",
    "\n",
    "\n",
    "def count_corpus(tokens):\n",
    "    \"\"\"count the frequency of token\"\"\"\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(),\n",
    "                                   key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {\n",
    "            token: idx\n",
    "            for idx, token in enumerate(self.idx_to_token)\n",
    "        }\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            # get 方法，先找第一个字符，找不到再找第二个字符\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):  # Index for the unknown token\n",
    "        return self._token_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b1fee-66ba-429a-a17d-823a207d97d3",
   "metadata": {},
   "source": [
    "# data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c4b7630-f32e-49f4-b6df-da9e0e4b8838",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T07:52:43.816572Z",
     "start_time": "2022-04-02T07:52:43.781554Z"
    }
   },
   "outputs": [],
   "source": [
    "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
    "    \"\"\"Generate a minibatch of subsequences using random sampling.\n",
    "\n",
    "    Defined in :numref:`sec_language_model`\"\"\"\n",
    "    # Start with a random offset (inclusive of `num_steps - 1`) to partition a\n",
    "    # sequence\n",
    "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    # Subtract 1 since we need to account for labels\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "    # The starting indices for subsequences of length `num_steps`\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # In random sampling, the subsequences from two adjacent random\n",
    "    # minibatches during iteration are not necessarily adjacent on the\n",
    "    # original sequence\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        # Return a sequence of length `num_steps` starting from `pos`\n",
    "        return corpus[pos:pos + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # Here, `initial_indices` contains randomized starting indices for\n",
    "        # subsequences\n",
    "        initial_indices_per_batch = initial_indices[i:i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
    "    \"\"\"Generate a minibatch of subsequences using sequtial partitioning \"\"\"\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = (\n",
    "        (len(corpus) - offset - 1) // batch_size) * batch_size  # 目前的总tokens个数\n",
    "    Xs = torch.tensor(\n",
    "        corpus[offset:offset +\n",
    "               num_tokens])  # offset + num_tokens == len corpus -1 倒数第二个元素\n",
    "    Ys = torch.tensor(corpus[offset + 1:offset + num_tokens +\n",
    "                             1])  # offset + num_tokens + 1 是corpus最后一个元素\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)  #\n",
    "    num_batches = Xs.shape[1] // num_steps  # 每个batch中的东西\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i:i + num_steps]\n",
    "        Y = Ys[:, i:i + num_steps]\n",
    "        yield X, Y\n",
    "\n",
    "\n",
    "class SeqDataLoader:\n",
    "    \"\"\"An iterator to load sequence data.\"\"\"\n",
    "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
    "        \"\"\"Defined in :numref:`sec_language_model`\"\"\"\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "        self.corpus, self.vocab = load_corpus_stone(max_tokens)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n",
    "    \n",
    "def load_corpus_stone(max_tokens=-1):\n",
    "    \"\"\"Return token indices and the vocabulary of the stone\"\"\"\n",
    "    lines = read_chinese_file(\"红楼梦.txt\")\n",
    "    tokens = tokenize(lines)\n",
    "    vocab = Vocab(tokens, min_freq=0)\n",
    "    # since each text line in the stone dataset is not necessarily\n",
    "    # a sentence or a paragraph, flatten all the text lines into a single list\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "\n",
    "def load_data_stone_language_model(batch_size,\n",
    "                    num_steps,\n",
    "                    use_random_iter=False,\n",
    "                    max_tokens=10000):\n",
    "    \"\"\"Return the iterator and the vocabulary of the stone dataset.\"\"\"\n",
    "    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter,\n",
    "                              max_tokens)\n",
    "    return data_iter, data_iter.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcfcc38-ff1e-4643-8cb1-4f065c14d165",
   "metadata": {},
   "source": [
    "# 模型函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41f62b48-4a28-4725-aaf8-5407e8a0042c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T07:52:46.204930Z",
     "start_time": "2022-04-02T07:52:46.177613Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size # 隐藏层参数在rnn层中\n",
    "        \n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "    # inputs 是 batch_size, time, 转成 time, batch_size, 再用one_hot 拉成 time, batch_size, vocab_size\n",
    "    def forward(self, inputs, states):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, states = self.rnn(X, states)\n",
    "        # output 最后拉成 time*batch_size * vocab_size的向量，\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, states\n",
    "    \n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # scratch实现时， 隐藏层为1层， 无第一维，  就是batch_size, num_hiddens维\n",
    "            return torch.zeros( (self.num_directions * self.rnn.num_layers, batch_size,\n",
    "                              self.num_hiddens), device=device)\n",
    "        else: \n",
    "            # lstm中有两个隐藏变量， C与H， GRU与RNN都只有一个隐藏单元。 只需要一个隐藏权重\n",
    "            return (torch.zeros( (self.num_directions * self.rnn.num_layers, batch_size,\n",
    "                              self.num_hiddens), device=device),\n",
    "                    torch.zeros( (self.num_directions * self.rnn.num_layers, batch_size,\n",
    "                              self.num_hiddens), device=device))\n",
    "        \n",
    "def grad_clipping(net, theta):\n",
    "    \"\"\"Clip the gradient.\n",
    "\n",
    "    Defined in :numref:`sec_rnn_scratch`\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm\n",
    "\n",
    "            \n",
    "def try_gpu(i=0):\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().`\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b18e8d-45e2-4f15-a8a8-1ba8e5247a8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403acbd8-eac7-4e6e-83d3-c623d78f407d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T07:52:48.757785Z",
     "start_time": "2022-04-02T07:52:48.738836Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    state, timer = None, Timer()\n",
    "    metric = Accumulator(2)\n",
    "    # X shape [batch_size, time, vocab_size]\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                state.detach_()\n",
    "            else:\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        # label.shape = batch_size, num_steps, \n",
    "        y = Y.T.reshape(-1) \n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        \n",
    "        updater.zero_grad()\n",
    "        l.backward()\n",
    "        grad_clipping(net, 1)\n",
    "        updater.step()\n",
    "        metric.add(l * torch.numel(y), torch.numel(y))\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()\n",
    "\n",
    "def train(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = Animator(xlabel='epoch',\n",
    "                            ylabel='perplexity',\n",
    "                            legend=['train'],\n",
    "                            xlim=[10, num_epochs])\n",
    "    updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    in_predict = lambda prefix: predict(prefix, 50, net, vocab, device)\n",
    "    # Train and predict\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch(net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "    print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')\n",
    "    print(in_predict('time traveller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca9690c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prefix, num_preds, net, vocab, device):\n",
    "    \"\"\"Generate new characters following the `prefix`.\n",
    "    Defined in :numref:`sec_rnn_scratch`\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    # 第一个字符在vocab里的下标，并放到列表里\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    # get_input是一个小函数， 将output最后一个词拿出来，\n",
    "    # reshape成（1，1）的矩阵，batch_size = 1, num_steps=1\n",
    "    get_input = lambda: torch.reshape(torch.tensor([outputs[-1]], device=device), (1, 1))\n",
    "    \n",
    "    # 一个一个词做预测， 而且0的词已经放到了output中， 所以从1开始遍历，过一遍prefix。\n",
    "    for y in prefix[1:]:  # Warm-up period\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # Predict `num_preds` steps\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b957a3",
   "metadata": {},
   "outputs": [],
   "source": [
    " import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf82db-c53e-405e-8b1a-dc71b953aba7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 测试结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c0caf5-4f95-4dc1-9bc5-1d393d9c50a6",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "## max_tokens = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934cf8bd-4604-4b2c-89e0-35cb00efabff",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = load_data_stone(batch_size, num_steps, max_tokens=5000)\n",
    "\n",
    "vocab_size, num_hiddens, device = len(vocab), 256, try_gpu()\n",
    "num_epochs, lr = 500, 1\n",
    "\n",
    "lstm_layer = nn.LSTM(len(vocab), num_hiddens)\n",
    "model = RNNModel(lstm_layer, len(vocab))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d68da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"黛玉\", 100, model, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc78be-60d9-4007-9e1c-ab82bec7cd97",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(model, train_iter, vocab, lr, 10, device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d0e1e-2f12-417b-8ce8-884ac6ad6eb5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(model, train_iter, vocab, lr, num_epochs, device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0664447c-5a91-4872-a92a-4b8fdd70d055",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_samples = 400\n",
    "article = str()\n",
    "state = (torch.zeros(size=(1,1, num_hiddens), device=device), torch.zeros(size=(1,1, num_hiddens), device=device))\n",
    "for i in state:\n",
    "    i = i.to(device)\n",
    "prob = torch.ones([vocab_size])\n",
    "_input = torch.multinomial(prob, num_samples=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a4071-2551-4a0e-b2a9-ad4e809a8c2d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## max_tokens = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee93b708-a75d-48e3-8458-988e0e21a414",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = load_data_stone(batch_size, num_steps, max_tokens=20000)\n",
    "\n",
    "vocab_size, num_hiddens, device = len(vocab), 256, try_gpu()\n",
    "num_epochs, lr = 1500, 1\n",
    "\n",
    "lstm_layer = nn.LSTM(len(vocab), num_hiddens)\n",
    "model = RNNModel(lstm_layer, len(vocab))\n",
    "model = model.to(device)\n",
    "train(model, train_iter, vocab, lr, num_epochs, device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7b1a95-3b6e-4ba7-bc46-e642b4d20eaf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "train(model, train_iter, vocab, lr, num_epochs,  device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42a320-9f0c-4273-b912-aa40b8ba2b53",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_samples = 400\n",
    "article = str()\n",
    "state = (torch.zeros(size=(1,1, num_hiddens), device=device), torch.zeros(size=(1,1, num_hiddens), device=device))\n",
    "for i in state:\n",
    "    i = i.to(device)\n",
    "prob = torch.ones([vocab_size])\n",
    "_input = torch.multinomial(prob, num_samples=1).unsqueeze(1)\n",
    "for i in range(num_samples):\n",
    "    _input = _input.to(device)\n",
    "    #print(_input)\n",
    "    output, state = model(_input, state)\n",
    "    #print(_input)\n",
    "    # prob是对上一步得到的output进行指数化，加强高概率结果的权重；\n",
    "    prob = output.exp()\n",
    "    # word_id，通过torch_multinomial，以prob为权重，对结果进行加权抽样，样本数为1(即num_samples)\n",
    "    word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "    # 为下一次运算作准备，通过fill_方法，把最新的结果(word_id)作为_input的值\n",
    "    _input.fill_(word_id)\n",
    "    _input = _input.to(device)\n",
    "    # print(_input)\n",
    "    # 从字典映射表Dictionary里，找到当前索引(即word_id)对应的单词；\n",
    "    word = vocab.idx_to_token[word_id]\n",
    "    # 如果获得到的单词是特殊符号(如<eos>，句尾符号EndOfSentence)，替换成换行符\n",
    "    word = '\\n' if word == '<eos>' else word\n",
    "    article += word\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2507b14-316c-439a-b5c3-9b6e11a7ac28",
   "metadata": {},
   "source": [
    "#  训练embedding预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f3ddd",
   "metadata": {},
   "source": [
    "## 下采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c425e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(sentences, vocab):\n",
    "    sentences = [[token for token in line if vocab[token] != vocab.unk] \n",
    "                 for line in sentences]\n",
    "    counter = count_corpus(sentences)\n",
    "    num_tokens = sum(counter.values()) \n",
    "    # 字典想找到所有的值，可以使用values\n",
    "    \n",
    "    def keep(token):\n",
    "        return (random.uniform(0, 1) <\n",
    "               math.sqrt(1e-4 / counter[token] * num_tokens))\n",
    "    \n",
    "    return ([[token for token in line if keep(token)] for line in sentences],\n",
    "           counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bfb7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = read_chinese_file(\"红楼梦.txt\")\n",
    "tokens = tokenize(lines)\n",
    "vocab = Vocab(tokens, min_freq=5)\n",
    "\n",
    "subsampled, counter = subsample(tokens, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd6626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compare_counts(token):\n",
    "    return (f'\"{token}\"的数量：'\n",
    "            f'之前={sum([l.count(token) for l in tokens])}, '\n",
    "            f'之后={sum([l.count(token) for l in subsampled])}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b48bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_counts('：')\n",
    "sum([l.count(\"宝钗\") for l in tokens]) \n",
    "corpus = [vocab[line] for line in subsampled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9584a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(corpus, max_window_size):\n",
    "    # 返回skip gram的中心词与上下文\n",
    "    centers, contexts = [], []\n",
    "    for line in corpus:\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        centers += line \n",
    "        # centers 为 许多center组成的一维列表\n",
    "        for i in range(len(line)):\n",
    "            # 为什么window_size取随机？\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, i-window_size)\n",
    "                                    ,min(len(line), i + 1 + window_size)))\n",
    "            # 去除中心词\n",
    "            indices.remove(i)\n",
    "            # contexts添加的是一个列表，列表里存着所有上下文的下标\n",
    "            contexts.append([line[idx] for idx in indices])\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53eb75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
    "print('数据集', tiny_dataset)\n",
    "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
    "    print('中心词', center, '的上下文词是', context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8977589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "temp += [torch.arange(10)]\n",
    "temp += [torch.arange(20)]\n",
    "temp += [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb8e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp,torch.arange(20), [torch.arange(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(corpus, 5)\n",
    "f'# “中心词-上下文词对”的数量: {sum([len(contexts) for contexts in all_contexts])}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e9e90",
   "metadata": {},
   "source": [
    "## 负采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f825044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomGenerator:\n",
    "    \"\"\"根据n个采样权重在{1，...，n}中随机选取\"\"\"\n",
    "    def __init__(self, sampling_weights):\n",
    "        self.population = list(range(1, len(sampling_weights)+1))\n",
    "        self.sampling_weights = sampling_weights\n",
    "        self.candidates = []\n",
    "        self.i = 0\n",
    "    \n",
    "    def draw(self):\n",
    "        # 每次选取10000个点， 到10000之后再重新选取\n",
    "        if self.i == len(self.candidates):\n",
    "            self.candidates = random.choices(\n",
    "                self.population, self.sampling_weights, k=10000)\n",
    "            self.i = 0\n",
    "        self.i += 1\n",
    "        return self.candidates[self.i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d39a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = RandomGenerator([2, 3, 0])\n",
    "[generator.draw() for _ in range(10)]\n",
    "# 传入权重， 在该列表中，根据权重返回下标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec742989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts, vocab, counter, K):\n",
    "    \"\"\"返回负采样中的噪声词\"\"\"\n",
    "    # 每个词的采样频率为 counter中的概率的0.75次方\n",
    "    sampling_weights = [counter[vocab.to_tokens(i)]**0.75\n",
    "                       for i in range(1, len(vocab))]\n",
    "    all_negatives, generator = [], RandomGenerator(sampling_weights)\n",
    "    \n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            neg = generator.draw()\n",
    "            if neg not in contexts:\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494ec94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_negatives = get_negatives(all_contexts, vocab, counter, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12432a42",
   "metadata": {},
   "source": [
    "# 读取四元tuple小批量\n",
    "\n",
    "原始数据经过下采样预处理, 返回的样本为增加负采样的样本\n",
    "\n",
    "方法返回tuple(conters, contexts_negatives, masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ed893aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data):\n",
    "    \"\"\"返回带有负采样的skipgram的小批量样本\"\"\"\n",
    "    max_len = max(len(c) + len(n) for _, c , n in data) \n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        # center为int， 所以在+=时需要变列表 而且最后需要reshape\n",
    "        centers += [center]\n",
    "        contexts_negatives += \\\n",
    "            [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        # labels 就是判断模型是否判断正确的模型，1即为context，0则反之\n",
    "        labels += [[1] * len(context) + [0] * (max_len-len(context))]\n",
    "    # 忘了reshape centers 出错一次\n",
    "    return (torch.tensor(centers).reshape((-1,1)), torch.tensor(\n",
    "    contexts_negatives), torch.tensor(masks), torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "45f43806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers = tensor([[1],\n",
      "        [2]])\n",
      "contexts_negatives = tensor([[2, 2, 3, 3, 3, 3],\n",
      "        [2, 2, 2, 3, 3, 0]])\n",
      "masks = tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0]])\n",
      "labels = tensor([[1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x_1 = (1, [2, 2], [3, 3, 3, 3])\n",
    "x_2 = (2, [2, 2, 2], [3, 3])\n",
    "batch = batchify((x_1, x_2))\n",
    "\n",
    "names = ['centers', 'contexts_negatives', 'masks', 'labels']\n",
    "for name, data in zip(names, batch):\n",
    "    print(name, '=', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82cfea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def load_data_stone_embedding(batch_size, max_window_size, num_noise_words):\n",
    "    \"\"\"将红楼梦加载到内存中\"\"\"\n",
    "    num_workers = 0\n",
    "    lines = read_chinese_file(\"红楼梦.txt\")\n",
    "    sentences = tokenize(lines)\n",
    "    vocab = Vocab(sentences, min_freq=10)\n",
    "    subsampled, counter = subsample(sentences, vocab)\n",
    "    corpus = [vocab[line] for line in subsampled]\n",
    "    all_centers, all_contexts = get_centers_and_contexts(corpus, max_window_size)\n",
    "    all_negatives =  get_negatives(all_contexts, vocab, counter, num_noise_words)\n",
    "    \n",
    "    class STONEDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, centers, contexts, negatives):\n",
    "            assert len(centers) == len(contexts) == len(negatives)\n",
    "            self.centers = centers\n",
    "            self.contexts = contexts\n",
    "            self.negatives = negatives\n",
    "        \n",
    "        def __getitem__(self, index):\n",
    "            return (self.centers[index], self.contexts[index],\n",
    "                   self.negatives[index])\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.centers)\n",
    "        \n",
    "    dataset = STONEDataset(all_centers, all_contexts, all_negatives)\n",
    "    # 最后的batchify 返回的是四个元素组成的tuple(center, context_neg, mask, label)\n",
    "    data_iter = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size, shuffle=True,\n",
    "        collate_fn=batchify, num_workers=num_workers)\n",
    "    return data_iter, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b8c24e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter, vocab = load_data_stone_embedding(512, 5, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34214cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: torch.Size([512, 1])\n",
      "contexts_negatives shape: torch.Size([512, 60])\n",
      "masks shape: torch.Size([512, 60])\n",
      "labels shape: torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "for batch in data_iter:\n",
    "    for name, data in zip(names, batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf06b4b",
   "metadata": {},
   "source": [
    "# word2vec pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f212910",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\wplf\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.478 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "batch_size, max_window_size, num_noise_words = 512, 5, 5\n",
    "data_iter, vocab = load_data_stone_embedding(batch_size, max_window_size, num_noise_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72fa444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(num_embeddings=20, embedding_dim=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "735f470c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter embedding_weight (torch.Size([20, 4]), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "print(f'Parameter embedding_weight ({embed.weight.shape}, '\n",
    "      f'dtype={embed.weight.dtype})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c5a8a2",
   "metadata": {},
   "source": [
    "## 前向传播\n",
    "在前向传播中，跳元语法模型的输入包括形状为（批量大小，1）的中心词索引center和形状为（批量大小，max_len）的上下文与噪声词索引contexts_and_negatives，其中max_len在 :numref:subsec_word2vec-minibatch-loading中定义。这两个变量首先通过嵌入层从词元索引转换成向量，然后它们的批量矩阵相乘（在 :numref:subsec_batch_dot中描述）返回形状为（批量大小，1，max_len）的输出。输出中的每个元素是中心词向量和上下文或噪声词向量的点积。\n",
    "\n",
    "**输出很关键,点积代表center与上下文或噪声的相似度**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "352f7189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    # 利用中心词与context的点积代表相似度\n",
    "    v = embed_v(center)\n",
    "    u = embed_u(contexts_and_negatives) \n",
    "    return torch.bmm(v, u.permute(0, 2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c075558",
   "metadata": {},
   "source": [
    "## 二元交叉熵损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52005d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6.0000, 11.5000, 17.0000],\n",
       "         [ 2.2000,  2.0000,  6.5000]]),\n",
       " tensor([[ 2.2000,  2.0000,  6.5000],\n",
       "         [ 6.0000, 11.5000, 17.0000]]),\n",
       " torch.Size([2, 2, 3]),\n",
       " tensor([[ 2.0000,  5.1333],\n",
       "         [21.0000,  2.0000]]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor( [[ [1.0, 2.0, 3.0], [3.4, 2, 10.0] ], [ [11.0, 21.0, 31.0],[1.0, 2.0, 3.0]]])\n",
    "a.mean(dim=0), a.mean(dim=1),a.shape, a.mean(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c03456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, inputs, target, mask=None):\n",
    "        out = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, target, weight=mask, reduction=\"none\")\n",
    "        # out.mean(dim=1, )\n",
    "        # input shape(batch_size, category), 最后BCE出来是 二维的,第一维是个数,第二维是个数, 所以在第dim=1求mean\n",
    "        return out.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5e046ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.9352, 1.8462]),\n",
       " torch.Size([2, 4]),\n",
       " torch.Size([2, 4]),\n",
       " torch.Size([2, 4]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = SigmoidBCELoss()\n",
    "pred = torch.tensor( [[1.1, -2.2, 3.3, -4.4]] * 2)\n",
    "label = torch.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]])\n",
    "# 这里的分类是n个二分类,不是n维的softmax多分类!!!!!\n",
    "mask = torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0]])\n",
    "loss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1), pred.shape, label.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "637dca9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9352\n",
      "1.8462\n"
     ]
    }
   ],
   "source": [
    "def sigmd(x):\n",
    "    # sigmoid(x) = 1 / (1 + exp(-x))\n",
    "    # -log 即为交叉熵\n",
    "    return -math.log(1 / (1 + math.exp(-x)))\n",
    "# 当例子为1时的 +x的损失与例子为0时的 -x的损失相同, 还真是相等, 李沐是真滴牛皮!!!!!!\n",
    "\n",
    "print(f'{(sigmd(1.1) + sigmd(2.2) + sigmd(-3.3) + sigmd(4.4)) / 4:.4f}')\n",
    "print(f'{(sigmd(-1.1) + sigmd(-2.2)) / 2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c16c98",
   "metadata": {},
   "source": [
    "## word2vec skip gram训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82aaf082",
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "#定义两个嵌入层, 分别给center与context做embeddding\n",
    "net = nn.Sequential(nn.Embedding(num_embeddings=len(vocab),\n",
    "                                embedding_dim=embed_size),\n",
    "                   nn.Embedding(num_embeddings=len(vocab),\n",
    "                                embedding_dim=embed_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a7b80c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data_iter, lr, num_epochs, device = try_gpu()):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Embedding:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    animator = Animator(xlabel='epoch', ylabel='loss',\n",
    "                       xlim=[1, num_epochs])\n",
    "    # 规范化的损失之和, 规范化的损失数\n",
    "    metric = Accumulator(2)\n",
    "    for epoch in range(num_epochs):\n",
    "        timer, num_batches = Timer(), len(data_iter)\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            optimizer.zero_grad()\n",
    "            center, context_negative, mask, label = [\n",
    "                data.to(device) for data in batch]\n",
    "            # net中的两层分别用来给center, context_negative做embedding\n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "            l = (loss(pred.reshape(label.shape).float(), label.float(), mask)\n",
    "                / mask.sum(axis=1) * mask.shape[1])\n",
    "            l.sum().backward()\n",
    "            optimizer.step()\n",
    "            metric.add(l.sum(), l.numel())\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[1],))\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, '\n",
    "          f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3a05e352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.403, 181128.0 tokens/sec on cpu\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr, num_epochs = 0.002, 10\n",
    "train(net, data_iter, lr, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b8f4ff53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与吃饭相似的词为:三四个,\n",
      " cosine相似度=0.568\n",
      "\n",
      "与吃饭相似的词为:沏,\n",
      " cosine相似度=0.531\n",
      "\n",
      "与吃饭相似的词为:粥,\n",
      " cosine相似度=0.527\n",
      "\n",
      "与吃饭相似的词为:回过,\n",
      " cosine相似度=0.525\n",
      "\n",
      "与吃饭相似的词为:殷勤,\n",
      " cosine相似度=0.516\n",
      "\n",
      "与吃饭相似的词为:罢,\n",
      " cosine相似度=0.511\n",
      "\n",
      "与吃饭相似的词为:坐下,\n",
      " cosine相似度=0.505\n",
      "\n",
      "与吃饭相似的词为:坐车,\n",
      " cosine相似度=0.504\n",
      "\n",
      "与吃饭相似的词为:桌上,\n",
      " cosine相似度=0.500\n",
      "\n",
      "与吃饭相似的词为:依言,\n",
      " cosine相似度=0.499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[vocab[query_token]]\n",
    "    cos = torch.mv(W, x) / torch.sqrt(torch.sum(W * W, dim=1) * \n",
    "                                     torch.sum(x * x) + 1e-9)\n",
    "    topk = torch.topk(cos, k=k+1)[1].cpu().numpy().astype('int32')\n",
    "    for i in topk[1:]:\n",
    "        print(f'与{query_token}相似的词为:{vocab.to_tokens(i)},\\n cosine相似度={float(cos[i]):.3f}\\n')\n",
    "get_similar_tokens('吃饭', 10, net[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3efa55f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

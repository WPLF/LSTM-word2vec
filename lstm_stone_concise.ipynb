{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56958fcf-f363-421c-9200-8f73383f17f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T07:52:31.972294Z",
     "start_time": "2022-04-02T07:52:27.694843Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import jieba\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8176086-d128-4353-8c05-20dac9359ea7",
   "metadata": {},
   "source": [
    "# 通用module， timer， animator， accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2da9810d-7c1c-4909-a07f-0ddec5a0c6e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T07:52:37.741980Z",
     "start_time": "2022-04-02T07:52:37.713881Z"
    }
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Defined in :numref:`subsec_linear_model`\"\"\"\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "    \n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"Set the axes for matplotlib.\n",
    "\n",
    "    Defined in :numref:`sec_calculus`\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()\n",
    "    \n",
    "    \n",
    "class Animator:\n",
    "    \"\"\"For plotting data in animation.\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        \"\"\"Defined in :numref:`sec_softmax_scratch`\"\"\"\n",
    "        # Incrementally plot multiple lines\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # Use a lambda function to capture arguments\n",
    "        self.config_axes = lambda: set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # Add multiple data points into the figure\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "class Accumulator:\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        \"\"\"Defined in :numref:`sec_softmax_scratch`\"\"\"\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2ef0e8-547c-4744-8697-9031f483979a",
   "metadata": {},
   "source": [
    "# nlp vocab与读文件的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552603de-8d9c-41b0-9f27-6e97eddf1fe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T07:52:41.504208Z",
     "start_time": "2022-04-02T07:52:41.470306Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_chinese_file(path):\n",
    "    \"\"\"Load the chinese file into a list of text lines.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('\\s', ' ', line).strip() for line in lines]\n",
    "\n",
    "\n",
    "def tokenize(lines):\n",
    "    \"\"\"根据jieba分词将lines分成tokens\"\"\"\n",
    "    return [jieba.lcut(line) for line in lines]\n",
    "\n",
    "\n",
    "def count_corpus(tokens):\n",
    "    \"\"\"count the frequency of token\"\"\"\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(),\n",
    "                                   key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {\n",
    "            token: idx\n",
    "            for idx, token in enumerate(self.idx_to_token)\n",
    "        }\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            # get 方法，先找第一个字符，找不到再找第二个字符\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):  # Index for the unknown token\n",
    "        return self._token_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b1fee-66ba-429a-a17d-823a207d97d3",
   "metadata": {},
   "source": [
    "# data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c4b7630-f32e-49f4-b6df-da9e0e4b8838",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T07:52:43.816572Z",
     "start_time": "2022-04-02T07:52:43.781554Z"
    }
   },
   "outputs": [],
   "source": [
    "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
    "    \"\"\"Generate a minibatch of subsequences using random sampling.\n",
    "\n",
    "    Defined in :numref:`sec_language_model`\"\"\"\n",
    "    # Start with a random offset (inclusive of `num_steps - 1`) to partition a\n",
    "    # sequence\n",
    "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    # Subtract 1 since we need to account for labels\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "    # The starting indices for subsequences of length `num_steps`\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # In random sampling, the subsequences from two adjacent random\n",
    "    # minibatches during iteration are not necessarily adjacent on the\n",
    "    # original sequence\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        # Return a sequence of length `num_steps` starting from `pos`\n",
    "        return corpus[pos:pos + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # Here, `initial_indices` contains randomized starting indices for\n",
    "        # subsequences\n",
    "        initial_indices_per_batch = initial_indices[i:i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
    "    \"\"\"Generate a minibatch of subsequences using sequtial partitioning \"\"\"\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = (\n",
    "        (len(corpus) - offset - 1) // batch_size) * batch_size  # 目前的总tokens个数\n",
    "    Xs = torch.tensor(\n",
    "        corpus[offset:offset +\n",
    "               num_tokens])  # offset + num_tokens == len corpus -1 倒数第二个元素\n",
    "    Ys = torch.tensor(corpus[offset + 1:offset + num_tokens +\n",
    "                             1])  # offset + num_tokens + 1 是corpus最后一个元素\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)  #\n",
    "    num_batches = Xs.shape[1] // num_steps  # 每个batch中的东西\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i:i + num_steps]\n",
    "        Y = Ys[:, i:i + num_steps]\n",
    "        yield X, Y\n",
    "\n",
    "\n",
    "class SeqDataLoader:\n",
    "    \"\"\"An iterator to load sequence data.\"\"\"\n",
    "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
    "        \"\"\"Defined in :numref:`sec_language_model`\"\"\"\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "        self.corpus, self.vocab = load_corpus_stone(max_tokens)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n",
    "    \n",
    "def load_corpus_stone(max_tokens=-1):\n",
    "    \"\"\"Return token indices and the vocabulary of the stone\"\"\"\n",
    "    lines = read_chinese_file(\"红楼梦.txt\")\n",
    "    tokens = tokenize(lines)\n",
    "    vocab = Vocab(tokens, min_freq=0)\n",
    "    # since each text line in the stone dataset is not necessarily\n",
    "    # a sentence or a paragraph, flatten all the text lines into a single list\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "\n",
    "def load_data_stone(batch_size,\n",
    "                    num_steps,\n",
    "                    use_random_iter=False,\n",
    "                    max_tokens=10000):\n",
    "    \"\"\"Return the iterator and the vocabulary of the stone dataset.\"\"\"\n",
    "    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter,\n",
    "                              max_tokens)\n",
    "    return data_iter, data_iter.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcfcc38-ff1e-4643-8cb1-4f065c14d165",
   "metadata": {},
   "source": [
    "# 模型函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41f62b48-4a28-4725-aaf8-5407e8a0042c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T07:52:46.204930Z",
     "start_time": "2022-04-02T07:52:46.177613Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size # 隐藏层参数在rnn层中\n",
    "        \n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "    # inputs 是 batch_size, time, 转成 time, batch_size, 再用one_hot 拉成 time, batch_size, vocab_size\n",
    "    def forward(self, inputs, states):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, states = self.rnn(X, states)\n",
    "        # output 最后拉成 time*batch_size * vocab_size的向量，\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, states\n",
    "    \n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # scratch实现时， 隐藏层为1层， 无第一维，  就是batch_size, num_hiddens维\n",
    "            return torch.zeros( (self.num_directions * self.rnn.num_layers, batch_size,\n",
    "                              self.num_hiddens), device=device)\n",
    "        else: \n",
    "            # lstm中有两个隐藏变量， C与H， GRU与RNN都只有一个隐藏单元。 只需要一个隐藏权重\n",
    "            return (torch.zeros( (self.num_directions * self.rnn.num_layers, batch_size,\n",
    "                              self.num_hiddens), device=device),\n",
    "                    torch.zeros( (self.num_directions * self.rnn.num_layers, batch_size,\n",
    "                              self.num_hiddens), device=device))\n",
    "        \n",
    "def grad_clipping(net, theta):\n",
    "    \"\"\"Clip the gradient.\n",
    "\n",
    "    Defined in :numref:`sec_rnn_scratch`\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm\n",
    "\n",
    "            \n",
    "def try_gpu(i=0):\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().`\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b18e8d-45e2-4f15-a8a8-1ba8e5247a8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403acbd8-eac7-4e6e-83d3-c623d78f407d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T07:52:48.757785Z",
     "start_time": "2022-04-02T07:52:48.738836Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    state, timer = None, Timer()\n",
    "    metric = Accumulator(2)\n",
    "    for X, Y in train_iter:  # X shape [batch_size, time, vocab_size]\n",
    "        if state is None or use_random_iter:\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                state.detach_()\n",
    "            else:\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        # label, shape = batch_size, num_steps, \n",
    "        y = Y.T.reshape(-1) \n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        \n",
    "        updater.zero_grad()\n",
    "        l.backward()\n",
    "        grad_clipping(net, 1)\n",
    "        updater.step()\n",
    "        metric.add(l * torch.numel(y), torch.numel(y))\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()\n",
    "\n",
    "def train(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = Animator(xlabel='epoch',\n",
    "                            ylabel='perplexity',\n",
    "                            legend=['train'],\n",
    "                            xlim=[10, num_epochs])\n",
    "    updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    # predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
    "    # Train and predict\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch(net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "    print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9690c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ch8(prefix, num_preds, net, vocab, device):\n",
    "    \"\"\"Generate new characters following the `prefix`.\n",
    "\n",
    "    Defined in :numref:`sec_rnn_scratch`\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: d2l.reshape(d2l.tensor(\n",
    "        [outputs[-1]], device=device), (1, 1))\n",
    "    for y in prefix[1:]:  # Warm-up period\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # Predict `num_preds` steps\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf82db-c53e-405e-8b1a-dc71b953aba7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 测试结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c0caf5-4f95-4dc1-9bc5-1d393d9c50a6",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "## max_tokens = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934cf8bd-4604-4b2c-89e0-35cb00efabff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = load_data_stone(batch_size, num_steps, max_tokens=10000)\n",
    "\n",
    "vocab_size, num_hiddens, device = len(vocab), 256, try_gpu()\n",
    "num_epochs, lr = 500, 1\n",
    "\n",
    "lstm_layer = nn.LSTM(len(vocab), num_hiddens)\n",
    "model = RNNModel(lstm_layer, len(vocab))\n",
    "model = model.to(device)\n",
    "train(model, train_iter, vocab, lr, num_epochs, device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc78be-60d9-4007-9e1c-ab82bec7cd97",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(model, train_iter, vocab, lr, num_epochs, device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d0e1e-2f12-417b-8ce8-884ac6ad6eb5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(model, train_iter, vocab, lr, num_epochs, device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0664447c-5a91-4872-a92a-4b8fdd70d055",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_samples = 400\n",
    "article = str()\n",
    "state = (torch.zeros(size=(1,1, num_hiddens), device=device), torch.zeros(size=(1,1, num_hiddens), device=device))\n",
    "for i in state:\n",
    "    i = i.to(device)\n",
    "prob = torch.ones([vocab_size])\n",
    "_input = torch.multinomial(prob, num_samples=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a98be1-5c7d-4a42-b1ed-097679a66ad2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_samples = 400\n",
    "article = str()\n",
    "state = (torch.zeros(size=(1,1, num_hiddens), device=device), torch.zeros(size=(1,1, num_hiddens), device=device))\n",
    "for i in state:\n",
    "    i = i.to(device)\n",
    "prob = torch.ones([vocab_size])\n",
    "_input = torch.multinomial(prob, num_samples=1).unsqueeze(1)\n",
    "for i in range(num_samples):\n",
    "    _input = _input.to(device)\n",
    "    #print(_input)\n",
    "    output, state = model(_input, state)\n",
    "    #print(_input)\n",
    "    # prob是对上一步得到的output进行指数化，加强高概率结果的权重；\n",
    "    prob = output.exp()\n",
    "    # word_id，通过torch_multinomial，以prob为权重，对结果进行加权抽样，样本数为1(即num_samples)\n",
    "    word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "    # 为下一次运算作准备，通过fill_方法，把最新的结果(word_id)作为_input的值\n",
    "    _input.fill_(word_id)\n",
    "    _input = _input.to(device)\n",
    "    # print(_input)\n",
    "    # 从字典映射表Dictionary里，找到当前索引(即word_id)对应的单词；\n",
    "    word = vocab.idx_to_token[word_id]\n",
    "    # 如果获得到的单词是特殊符号(如<eos>，句尾符号EndOfSentence)，替换成换行符\n",
    "    word = '\\n' if word == '<eos>' else word\n",
    "    article += word\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a4071-2551-4a0e-b2a9-ad4e809a8c2d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## max_tokens = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee93b708-a75d-48e3-8458-988e0e21a414",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = load_data_stone(batch_size, num_steps, max_tokens=20000)\n",
    "\n",
    "vocab_size, num_hiddens, device = len(vocab), 256, try_gpu()\n",
    "num_epochs, lr = 1500, 1\n",
    "\n",
    "lstm_layer = nn.LSTM(len(vocab), num_hiddens)\n",
    "model = RNNModel(lstm_layer, len(vocab))\n",
    "model = model.to(device)\n",
    "train(model, train_iter, vocab, lr, num_epochs, device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7b1a95-3b6e-4ba7-bc46-e642b4d20eaf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "train(model, train_iter, vocab, lr, num_epochs,  device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42a320-9f0c-4273-b912-aa40b8ba2b53",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_samples = 400\n",
    "article = str()\n",
    "state = (torch.zeros(size=(1,1, num_hiddens), device=device), torch.zeros(size=(1,1, num_hiddens), device=device))\n",
    "for i in state:\n",
    "    i = i.to(device)\n",
    "prob = torch.ones([vocab_size])\n",
    "_input = torch.multinomial(prob, num_samples=1).unsqueeze(1)\n",
    "for i in range(num_samples):\n",
    "    _input = _input.to(device)\n",
    "    #print(_input)\n",
    "    output, state = model(_input, state)\n",
    "    #print(_input)\n",
    "    # prob是对上一步得到的output进行指数化，加强高概率结果的权重；\n",
    "    prob = output.exp()\n",
    "    # word_id，通过torch_multinomial，以prob为权重，对结果进行加权抽样，样本数为1(即num_samples)\n",
    "    word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "    # 为下一次运算作准备，通过fill_方法，把最新的结果(word_id)作为_input的值\n",
    "    _input.fill_(word_id)\n",
    "    _input = _input.to(device)\n",
    "    # print(_input)\n",
    "    # 从字典映射表Dictionary里，找到当前索引(即word_id)对应的单词；\n",
    "    word = vocab.idx_to_token[word_id]\n",
    "    # 如果获得到的单词是特殊符号(如<eos>，句尾符号EndOfSentence)，替换成换行符\n",
    "    word = '\\n' if word == '<eos>' else word\n",
    "    article += word\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2507b14-316c-439a-b5c3-9b6e11a7ac28",
   "metadata": {},
   "source": [
    "# n-gram 训练embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4277c1-1a7a-4499-bd82-a3d303e355e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c425e562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
